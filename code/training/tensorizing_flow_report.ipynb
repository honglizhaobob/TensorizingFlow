{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorizing Flow Numerical Experiments Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "# import utility functions\n",
    "import utils\n",
    "from utils.flow_models import *\n",
    "from utils.target import *\n",
    "from utils.training import *\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "# set number of threads\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "# import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import I/O\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# helper function for unpacking reports\n",
    "def load_report(report_filename, verbose=True, debug=False):\n",
    "    \"\"\" Assumes:\n",
    "    'initial_loss', \n",
    "    'model_snapshots', \n",
    "    'training_loss', \n",
    "    'test_loss', \n",
    "    'post_training_samples', \n",
    "    'grad_norms' variables are contained. \n",
    "    \n",
    "    If debug is set to True, loads the simplified dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    report = scipy.io.loadmat(report_filename)\n",
    "    plt.figure(1, figsize=(12, 6))\n",
    "    train = report['training_loss'][0]\n",
    "    test = report['test_loss'][0]\n",
    "    if debug:\n",
    "        # get rid of zero values \n",
    "        train = train[train != 0]\n",
    "        test = test[test != 0]\n",
    "    num_epochs = len(train)\n",
    "    if not debug:\n",
    "        # initial loss\n",
    "        loss_initial = report['initial_loss'][0]\n",
    "        print(\"[#####] ( Initial Loss ) = {}\".format(loss_initial))\n",
    "    \n",
    "    plt.plot(range(num_epochs), train, label='Training')\n",
    "    if not debug:\n",
    "        plt.axhline(y=loss_initial, linestyle='-.', color='purple', label='Initial Loss')\n",
    "    plt.plot(range(num_epochs), test, label='Generalization')\n",
    "    plt.title(\"Loss Profile for {} Epochs\".format(num_epochs))\n",
    "    plt.xlabel('Epoch #'); plt.ylabel(\"Loss ( KL Divergence )\");\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot gradient behavior\n",
    "    plt.figure(2, figsize=(12, 6))\n",
    "    # get gradient information\n",
    "    grad_norms = report['grad_norms'][0]\n",
    "    if debug:\n",
    "        # get rid of zero values \n",
    "        grad_norms = grad_norms[grad_norms != 0]\n",
    "    plt.plot(range(num_epochs), grad_norms, color='blue')\n",
    "    plt.xlabel('Epoch #'); plt.ylabel(\"Norm of Model Gradient\");\n",
    "    plt.title(\"Gradient Profile for {} Epochs\".format(num_epochs))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    if verbose:\n",
    "        # report final profiles\n",
    "        print(\"[#####] Final Loss Values: ( Train ) = {}, ( Test ) = {}\".format(train[-1], test[-1]))\n",
    "    \n",
    "def compare(report_filename1, report_filename2, true_stats=None):\n",
    "    \"\"\" Plot comparison plots for tensorizing and normalizing flow. \n",
    "    Assumes report1 is tensorizing flow, report 2 is normalizing flow.\n",
    "    \"\"\"\n",
    "    # static data must exist\n",
    "    assert os.path.isfile(\"./utils/full_rank_stats.mat\"), \\\n",
    "    \"::True statistics must be computed before loading report. \"\n",
    "    \n",
    "    report1 = scipy.io.loadmat(report_filename1)\n",
    "    report2 = scipy.io.loadmat(report_filename2)\n",
    "    # compare loss profiles\n",
    "    train1 = report1['training_loss'][0]\n",
    "    test1 = report1['test_loss'][0]\n",
    "    train2 = report2['training_loss'][0]\n",
    "    test2 = report2['test_loss'][0]\n",
    "    num_epochs = len(train1)\n",
    "    loss_initial1 = report1['initial_loss'][0]\n",
    "    loss_initial2 = report2['initial_loss'][0]\n",
    "    print(\"=== Epoch 1 Tensorizing: Train = {}, Test = {}\".format(train1[0], test1[0]))\n",
    "    print(\"=== Epoch 1 Normalizing: Train = {}, Test = {}\".format(train2[0], test2[0]))\n",
    "    # subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6));\n",
    "    \n",
    "    \n",
    "    ax1.axhline(y=loss_initial1, linestyle='-.', color='purple', label='Initial Loss (Tensor)')\n",
    "    ax1.plot(range(num_epochs), test1, label='Generalization (Tensor)', color='blue')\n",
    "    \n",
    "    ax1.axhline(y=loss_initial2, linestyle='-.', color='black', label='Initial Loss (Normal)')\n",
    "    ax1.plot(range(num_epochs), test2, label='Generalization (Normal)', color='grey')\n",
    "    \n",
    "    ax1.set_title(\"Generalization Loss Profile Comparison for {} Epochs\".format(num_epochs))\n",
    "    ax1.set_xlabel('Epoch #'); ax1.set_ylabel(\"Loss ( KL Divergence )\");\n",
    "    ax1.legend()\n",
    "    ax1.grid(True);\n",
    "    \n",
    "    # zoomed in\n",
    "    ax2.plot(range(num_epochs)[int(0.3*num_epochs):], \\\n",
    "             test1[int(0.3*num_epochs):], label='Generalization (Tensor)', color='blue')\n",
    "    ax2.plot(range(num_epochs)[int(0.3*num_epochs):], \\\n",
    "             test2[int(0.3*num_epochs):], label='Generalization (Normal)', color='grey')\n",
    "    # load true statistics\n",
    "    if true_stats is not None:\n",
    "        assert true_stats in ['gl1d', 'gl2d', 'rosen', 'double_rosen']\n",
    "        # need to make sure data is generated\n",
    "        stats = scipy.io.loadmat('./utils/full_rank_stats.mat')\n",
    "        # optimal loss\n",
    "        optimal_loss = stats[true_stats]['loss'][0][0][0][0]\n",
    "        print(\"Optimum = {}\".format(optimal_loss))\n",
    "        print(\"::Displaying optimum loss for: {}, for initialization. \\n\\n\".format(true_stats))\n",
    "        print(\"::( Tensorizing ) % Reduction = {}, % Distance from Optimum = {}\"\n",
    "              .format( (loss_initial1 - test1[-1]) / loss_initial1, abs(optimal_loss - test1[-1]) / optimal_loss ))\n",
    "        print(\"::( Normalizing ) % Reduction = {}, % Distance from Optimum = {}\"\n",
    "              .format( (loss_initial2 - test2[-1]) / loss_initial2, abs(optimal_loss - test2[-1]) / optimal_loss ))\n",
    "        ax2.axhline(y=loss_initial1, linestyle='-.', color='purple', label='Initial Loss (TT)')\n",
    "        ax2.axhline(y=loss_initial2, linestyle='-.', color='black', label='Initial Loss (Normal)')\n",
    "        ax2.axhline(y=optimal_loss, linestyle='-.', color='green', label='Optimal Loss')\n",
    "            \n",
    "    \n",
    "    ax2.set_title(\"Generalization Loss Profile Comparison for {} Epochs (Zoomed In)\".format(num_epochs))\n",
    "    ax2.set_xlabel('Epoch #'); ax2.set_ylabel(\"Loss ( KL Divergence )\");\n",
    "    # focus \n",
    "    if true_stats == 'gl1d':\n",
    "        ax2.set_ylim(60, 90)\n",
    "    if true_stats == 'gl2d':\n",
    "        ax2.set_ylim(60, 100)\n",
    "    if true_stats == 'rosen':\n",
    "        ax2.set_ylim(120, 140)\n",
    "    if true_stats == 'double_rosen':\n",
    "        ax2.set_ylim(118, 140)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// disable autoscrolling\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Results (Planar)\n",
    "\n",
    "Plotting loss profiles over 9 seeds. The following sections must be run sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GL 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train = 0\n",
    "avg_test = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    test_file = \"./report_aggregate/report/\" + \"seed{}_ginz1d_nf_planar_report.mat\".format(seed)\n",
    "    # load\n",
    "    data = scipy.io.loadmat(test_file)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    plt.title(\"Training Error\")\n",
    "    plt.figure(2)\n",
    "    plt.plot(np.arange(len(data['test_loss'][0])), data['test_loss'][0])\n",
    "    plt.title(\"Generalization Error\")\n",
    "    # averaged curve\n",
    "    avg_train = avg_train + data['training_loss'][0]\n",
    "    avg_test = avg_test + data['test_loss'][0]\n",
    "    \n",
    "# plot averaged curve\n",
    "avg_train_nf = avg_train / 10\n",
    "avg_test_nf = avg_test / 10\n",
    "plt.figure(3)\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_train_nf, label=\"( Train )\")\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_test_nf, label=\"( Test )\")\n",
    "print(\"> Averaged Final Training Loss = {}, Test Loss = {}\".format(avg_train_nf[-1], avg_test_nf[-1]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1_ginz1d_rk3_tf_resnet_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GL 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train = 0\n",
    "avg_test = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    test_file = \"./report_aggregate/report/\" + \"seed{}_ginz2d_nf_planar_report.mat\".format(seed)\n",
    "    # load\n",
    "    data = scipy.io.loadmat(test_file)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    plt.title(\"Training Error\")\n",
    "    plt.figure(2)\n",
    "    plt.plot(np.arange(len(data['test_loss'][0])), data['test_loss'][0])\n",
    "    plt.title(\"Generalization Error\")\n",
    "    # averaged curve\n",
    "    avg_train = avg_train + data['training_loss'][0]\n",
    "    avg_test = avg_test + data['test_loss'][0]\n",
    "    \n",
    "# plot averaged curve\n",
    "avg_train_nf = avg_train / 10\n",
    "avg_test_nf = avg_test / 10\n",
    "plt.figure(3)\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_train_nf, label=\"( Train )\")\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_test_nf, label=\"( Test )\")\n",
    "print(\"> Averaged Final Training Loss = {}, Test Loss = {}\".format(avg_train_nf[-1], avg_test_nf[-1]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train = 0\n",
    "avg_test = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    test_file = \"./report_aggregate/report/\" + \"seed{}_ginz2d_rk3_tf_planar_report.mat\".format(seed)\n",
    "    # load\n",
    "    data = scipy.io.loadmat(test_file)\n",
    "    plt.figure(1)\n",
    "    plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    plt.title(\"Training Error\")\n",
    "    plt.figure(2)\n",
    "    plt.plot(np.arange(len(data['test_loss'][0])), data['test_loss'][0])\n",
    "    plt.title(\"Generalization Error\")\n",
    "    # averaged curve\n",
    "    avg_train = avg_train + data['training_loss'][0]\n",
    "    avg_test = avg_test + data['test_loss'][0]\n",
    "    \n",
    "# plot averaged curve\n",
    "avg_train_tf = avg_train / 9\n",
    "avg_test_tf = avg_test / 9\n",
    "plt.figure(3)\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_train_tf, label=\"( Train )\")\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_test_tf, label=\"( Test )\")\n",
    "print(\"> Averaged Final Training Loss = {}, Test Loss = {}\".format(avg_train_tf[-1], avg_test_tf[-1]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_test_tf, label=\"Tensor\")\n",
    "plt.plot(np.arange(len(data['training_loss'][0])), avg_test_nf, label=\"Normal\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenbrock\n",
    "\n",
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_rosen_nf_planar_report.mat\".format(seed)\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_rosen_rk5_tf_planar_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 9\n",
    "avg_test_tf = avg_test_tensor / 9\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"Planar-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -4], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -4], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (8, 9)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (9, 10)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (10, 11)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[50:100], avg_test_tf[50:100], label=\"Tensor\")\n",
    "ax[1].plot(grid[50:100], avg_test_nf[50:100], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_double_rosen_nf_planar_report.mat\".format(seed)\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_double_rosen_rk4_tf_planar_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 9\n",
    "avg_test_tf = avg_test_tensor / 9\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"Planar-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -4], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -4], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (8, 9)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (9, 10)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (10, 11)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[50:], avg_test_tf[50:], label=\"Tensor\")\n",
    "ax[1].plot(grid[50:], avg_test_nf[50:], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Results (ResNet)\n",
    "\n",
    "Plotting loss profiles over 10 seeds. The following sections must be run sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GL 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 5):\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_ginz1d_rk3_tf_resnet_report.mat\".format(seed)\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_ginz1d_nf_resnet_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])[0:121]\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])[0:121]\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 4\n",
    "avg_test_tf = avg_test_tensor / 4\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"ResNet-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -24], \\\n",
    "            normal_final_samples['post_training_samples'][:, -23], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -24], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -23], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (23, 24)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -12], \\\n",
    "            normal_final_samples['post_training_samples'][:, -13], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -12], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -13], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (53, 54)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (63, 64)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Ginzburg-Landau 1D: Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[50:], avg_test_tf[50:], label=\"Tensor\")\n",
    "ax[1].plot(grid[50:], avg_test_nf[50:], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GL 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_ginz2d_nf_resnet_report.mat\".format(seed)\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_ginz2d_rk3_tf_resnet_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 9\n",
    "avg_test_tf = avg_test_tensor / 9\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"ResNet-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -24], \\\n",
    "            normal_final_samples['post_training_samples'][:, -23], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -24], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -23], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (23, 24)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -12], \\\n",
    "            normal_final_samples['post_training_samples'][:, -13], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -12], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -13], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (53, 54)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (63, 64)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Ginzburg-Landau 2D: Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[50:], avg_test_tf[50:], label=\"Tensor\")\n",
    "ax[1].plot(grid[50:], avg_test_nf[50:], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_test_tf[0], avg_test_tf[-1])\n",
    "print(avg_test_nf[0], avg_test_nf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_rosen_nf_resnet_report.mat\".format(seed)\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_rosen_rk5_tf_resnet_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 9\n",
    "avg_test_tf = avg_test_tensor / 9\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"ResNet-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -4], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -4], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (8, 9)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (9, 10)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (10, 11)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Rosenbrock: Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[:100], avg_test_tf[:100], label=\"Tensor\")\n",
    "ax[1].plot(grid[:100], avg_test_nf[:100], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_test_tf[0], min(avg_test_tf))\n",
    "print(avg_test_nf[0], min(avg_test_nf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Rosenbrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_normal = 0\n",
    "avg_test_tensor = 0\n",
    "for seed in np.arange(1, 10):\n",
    "    normal_file = \"./report_aggregate/report/\" + \"seed{}_double_rosen_nf_resnet_report.mat\".format(seed)\n",
    "    tensor_file = \"./report_aggregate/report/\" + \"seed{}_double_rosen_rk4_tf_resnet_report.mat\".format(seed)\n",
    "    # load\n",
    "    normal_data = scipy.io.loadmat(normal_file)\n",
    "    tensor_data = scipy.io.loadmat(tensor_file)\n",
    "    #plt.figure(1)\n",
    "    #plt.plot(np.arange(len(data['training_loss'][0])), data['training_loss'][0])\n",
    "    #plt.title(\"Training Error\")\n",
    "    plt.figure(2, figsize=(20, 8))\n",
    "    normal_test_loss = np.append(normal_data['initial_loss'][0], normal_data['test_loss'][0])\n",
    "    tensor_test_loss = np.append(tensor_data['initial_loss'][0], tensor_data['test_loss'][0])\n",
    "    #test_loss = data['test_loss'][0]\n",
    "    grid = np.arange(len(normal_test_loss))\n",
    "    #error = np.random.normal(0, 2, size=test_loss.shape)\n",
    "    plt.plot(grid, normal_test_loss, color='grey')\n",
    "    plt.plot(grid, tensor_test_loss, color='black')\n",
    "    #plt.fill_between(grid, test_loss-error, test_loss+error, color='grey')\n",
    "    plt.grid(True)\n",
    "    csfont = {'fontname':'Times New Roman'}\n",
    "    plt.title(\"Rosenbrock Test Err.\", **csfont)\n",
    "    # averaged curve\n",
    "    avg_test_tensor = avg_test_tensor + tensor_test_loss\n",
    "    avg_test_normal = avg_test_normal + normal_test_loss\n",
    "    \n",
    "# plot averaged curve\n",
    "#avg_train_nf = avg_train / 9\n",
    "avg_test_nf = avg_test_normal / 9\n",
    "avg_test_tf = avg_test_tensor / 9\n",
    "plt.figure(2)\n",
    "plt.plot(grid, avg_test_nf, label=\"( Normalizing Average )\", color='yellow')\n",
    "plt.plot(grid, avg_test_tf, label=\"( Tensorizing Average )\", color='orange')\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_nf[-1]))\n",
    "print(\"> Averaged Final Test Loss = {}\".format(avg_test_tf[-1]))\n",
    "plt.xlabel(\"Epoch\", **csfont)\n",
    "plt.ylabel(\"Generalization Loss\", **csfont)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Final Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_final_samples = normal_data\n",
    "tensor_final_samples = tensor_data\n",
    "\n",
    "plt.figure(3, figsize=(10, 8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "plt.suptitle(\"Planar-Normal: Post-Training\", **csfont)\n",
    "\n",
    "ax[0].scatter(normal_final_samples['post_training_samples'][:, -4], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue', label='normal');\n",
    "ax[0].scatter(tensor_final_samples['post_training_samples'][:, -4], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red', label='tensor');\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title(\"Dimension (8, 9)\")\n",
    "\n",
    "ax[1].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -3], s=1., color='blue');\n",
    "ax[1].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -3], s=1., color='red');\n",
    "ax[1].grid(True)\n",
    "ax[1].set_title(\"Dimension (9, 10)\")\n",
    "\n",
    "ax[2].scatter(normal_final_samples['post_training_samples'][:, -2], \\\n",
    "            normal_final_samples['post_training_samples'][:, -1], s=1., color='blue', label='normal');\n",
    "ax[2].scatter(tensor_final_samples['post_training_samples'][:, -2], \\\n",
    "            tensor_final_samples['post_training_samples'][:, -1], s=1., color='red', label='tensor');\n",
    "ax[2].legend()\n",
    "ax[2].grid(True)\n",
    "ax[2].set_title(\"Last Dimension (10, 11)\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "plt.suptitle(\"Double Rosenbrock Loss Profiles of Tensor vs. Normal\");\n",
    "ax[0].plot(grid, avg_test_tf, label=\"Tensor\")\n",
    "ax[0].plot(grid, avg_test_nf, label=\"Normal\")\n",
    "ax[0].set_title(\"Full Profile\")\n",
    "ax[0].set_ylabel(\"Test Loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(grid[50:], avg_test_tf[50:], label=\"Tensor\")\n",
    "ax[1].plot(grid[50:], avg_test_nf[50:], label=\"Normal\")\n",
    "ax[1].set_title(\"Zoomed In\")\n",
    "ax[1].set_ylabel(\"Test Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_test_tf[0], min(avg_test_tf))\n",
    "print(avg_test_nf[0], min(avg_test_nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all below\n",
    "stats = scipy.io.loadmat(\"./utils/full_rank_stats.mat\")\n",
    "stats['gl1d']['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"./data/gl1d_anti_full_rank.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X'].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X'].std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(data['X']).T\n",
    "likes = torch.log(torch.Tensor(data['likes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(data['likes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_init(x, prior_logpdf, targ_logpdf=utils.target.ginzburg_landau1d_anti_logpdf):\n",
    "    \"\"\" evaluate initial KL divergence between posterior distribution (NF + prior) \n",
    "    and target. x is samples without flow. This is a Monte-Carlo estimation of the \n",
    "    log partition function. \"\"\"\n",
    "    return (prior_logpdf - targ_logpdf(x)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"../../gl1d_anti_full_rank.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "R = data['R'][0]\n",
    "plt.plot(np.linspace(0, 1, 25), R*X[:,5], \"-o\", lw=2, color='orange'); plt.grid(True);\n",
    "plt.plot(np.linspace(0, 1, 25), R*X[:,4], \"-o\", lw=2, color='blue'); plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorizing flow dataset\n",
    "ginz_tf_dataset = utils.datasets.TensorizingFlowDataset(\"gl1d_samples_rk3.mat\")\n",
    "# initialize NF model\n",
    "ginz_flow = NormalizingFlow(dim=ginz_tf_dataset.dim, blocks=RESNET_BLOCKS2_GL1D, \\\n",
    "                            flow_length=6)\n",
    "# target PDF function\n",
    "ginz1d_log_pdf = ginzburg_landau1d_logpdf\n",
    "\n",
    "# begin training\n",
    "ginz1d_report = train(ginz_tf_dataset, ginz_flow, ginz1d_log_pdf, \n",
    "          num_epochs=100,\n",
    "          batch_size=256,\n",
    "          verbose=True,\n",
    "          lr=4e-4, \n",
    "          use_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"mixture_gaussian_nf_resnet_seed4.mat\")['report']\n",
    "data_tf = scipy.io.loadmat(\"mixture_gaussian_tf_resnet_seed4.mat\")['report']\n",
    "nf_test = data['test_loss'][0][0][0]\n",
    "tf_test = data_tf['test_loss'][0][0][0]\n",
    "print(nf_test[-1], tf_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nf_test, color='red')\n",
    "plt.plot(tf_test, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_samples = data['post_training_samples'][0][0]\n",
    "tf_samples = data_tf['post_training_samples'][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nf_samples[:, -2], nf_samples[:, -1], s=1.2, color='red')\n",
    "plt.scatter(tf_samples[:, -2], tf_samples[:, -1], s=1.2, color='blue')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
