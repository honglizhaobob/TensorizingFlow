{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference with Tensor-Train\n",
    "The general problem of Variational Inference (VI) is outlined as follows. Consider the joint density:\n",
    "$$\n",
    "    p(z,x) = p(z)\\cdot p(x|z)\n",
    "$$ where $z$ is latent variables and $x$ is observations. We would like to obtain an approximation to the posterior $p(z|x)$.\n",
    "\n",
    "We consider a family of probability densities $q(z)$ as an approximation to the true posterior $p(z|x)$. And minimize the KL (Kullback-Liebner) divergence:\n",
    "$$\n",
    "    \\min_q \\text{KL}(q(z) || p(z|x))\n",
    "$$\n",
    "\n",
    "In the normalizing flow setting, $p(z|x)$ is often the target density, $q(z)$ is the approximate density, often parametrized by some parameter class $\\theta$. We rewrite:\n",
    "$$\n",
    "    \\min_{\\theta} \\text{KL}(q(x;\\theta) || p(x)) = \\min_{\\theta}\\mathbb{E}_{x\\sim q(x;\\theta)}(\\log q(x;\\theta) - \\log p(x))\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume we have the ability to evaluate $p(x)$, but not necessarily sample from it. Furthermore, to get $x\\sim q(x;\\theta)$, the normalizing flow defines the transformation:\n",
    "$$\n",
    "    x = T(z) \n",
    "$$ with $z\\sim s(z)$ for some simple base distribution $s(z)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the change of variables formula:\n",
    "$$\n",
    "    q(x;\\theta) = s(z)\\cdot |\\det J_T(z)|^{-1}\n",
    "$$ where $J_T$ is the Jacobian of $T$ evaluated at $z$. This means we also need (and need only) the ability to sample from $z\\sim s(z)$. \n",
    "\n",
    "We have:\n",
    "$$\n",
    "    \\min_{\\theta}\\mathbb{E}_{z\\sim s(z)}(\\log s(z) - \\log|\\det J_T(z)| - \\log p(x))) \\approx \\min_{\\theta}\\frac{1}{N}\\sum_{i=1}^N\\bigg[\\big(\n",
    "    \\log s(z_i) - \\log|\\det J_{T_{\\theta}}(z_i)|\n",
    "    \\big) - \\log p(x_i)\\bigg]\n",
    "$$ where the last equality takes a sample dataset $\\{z_i\\}_{i=1}^N$ from the distribution $s(z)$. And $x_i = T(z_i)$. In particular, we do not require the ability to invert $T$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Considerations\n",
    "\n",
    "Consistent with the derivations above, we have the algorithm for training:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize flow model $T$, parametrized by parameter class $\\theta$\n",
    "\n",
    "* for $M$ epochs:\n",
    "\n",
    "    * draw $N$ sample data points $\\{z_i\\}_{i=1}^N$ from $s(z)$\n",
    "    \n",
    "        * evaluate $\\sum_{i=1}^N\\log s(z_i)$\n",
    "    \n",
    "    * flow the dataset to obtain $x_i = T(z_i)$\n",
    "        * evaluate $\\sum_{i=1}^N |\\det J_{T_{\\theta}}(z_i)|$\n",
    "        \n",
    "        * evaluate $\\sum_{i=1}^N p(x_i)$\n",
    "    * compute loss $KL = \\frac{1}{N}(\\sum_{i=1}^N \\log s(z_i) - \\sum_{i=1}^N |\\det J_{T_{\\theta}}(z_i)| - \\sum_{i=1}^N p(x_i))$\n",
    "    \n",
    "    * backpropogate (done automatically in PyTorch) and update $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing flow as correction to TT-IRT\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "\n",
    "# very high precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# return to original code: https://github.com/acids-ircam/pytorch_flows/blob/master/flows_04.ipynb\n",
    "\n",
    "# use normalizing flow as correction\n",
    "# cite: https://github.com/acids-ircam/pytorch_flows/blob/master/flows_04.ipynb\n",
    "class Flow(transform.Transform, nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "    \n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            # use random parameters\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "            \n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)\n",
    "    \n",
    "    # forward evaluation: x = f(z)\n",
    "    def forward(self, z):\n",
    "        pass\n",
    "\n",
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, dim, blocks, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            if blocks is None:\n",
    "                # by default uses Planar flow, which does not have inverse abiity\n",
    "                biject.append(PlanarFlow(dim))\n",
    "            else:\n",
    "                # alternate among the blocks\n",
    "                for flow in blocks:\n",
    "                    biject.append(flow(dim))\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        #self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "        return z, self.log_det\n",
    "\n",
    "\n",
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        psi = (1 - torch.tanh(f_z) ** 2) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-9)\n",
    "\n",
    "# Affine coupling flow\n",
    "class AffineCouplingFlow(Flow):\n",
    "    def __init__(self, dim, n_hidden=64, n_layers=3, activation=nn.ReLU):\n",
    "        super(AffineCouplingFlow, self).__init__()\n",
    "        self.k = dim // 2\n",
    "        self.g_mu = self.transform_net(self.k, dim - self.k, n_hidden, n_layers, activation)\n",
    "        self.g_sig = self.transform_net(self.k, dim - self.k, n_hidden, n_layers, activation)\n",
    "        self.init_parameters()\n",
    "        self.bijective = True\n",
    "\n",
    "    def transform_net(self, nin, nout, nhidden, nlayer, activation):\n",
    "        net = nn.ModuleList()\n",
    "        for l in range(nlayer):\n",
    "            net.append(nn.Linear(l==0 and nin or nhidden, l==nlayer-1 and nout or nhidden))\n",
    "            net.append(activation())\n",
    "        return nn.Sequential(*net)\n",
    "        \n",
    "    def _call(self, z):\n",
    "        z_k, z_D = z[:, :self.k], z[:, self.k:]\n",
    "        zp_D = z_D * torch.exp(self.g_sig(z_k)) + self.g_mu(z_k)\n",
    "        return torch.cat((z_k, zp_D), dim = 1)\n",
    "\n",
    "    def _inverse(self, z):\n",
    "        zp_k, zp_D = z[:, :self.k], z[:, self.k:]\n",
    "        z_D = (zp_D - self.g_mu(zp_k)) / self.g_sig(zp_k)\n",
    "        return torch.cat((zp_k, z_D))\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        z_k = z[:, :self.k]\n",
    "        return -torch.sum(torch.abs(self.g_sig(z_k)))\n",
    "    \n",
    "\n",
    "class ReverseFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ReverseFlow, self).__init__()\n",
    "        self.permute = torch.arange(dim-1, -1, -1)\n",
    "        self.inverse = torch.argsort(self.permute)\n",
    "\n",
    "    def _call(self, z):\n",
    "        return z[:, self.permute]\n",
    "\n",
    "    def _inverse(self, z):\n",
    "        return z[:, self.inverse]\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        return torch.zeros(z.shape[0], 1)\n",
    "    \n",
    "class ShuffleFlow(ReverseFlow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ShuffleFlow, self).__init__(dim)\n",
    "        self.permute = torch.randperm(dim)\n",
    "        self.inverse = torch.argsort(self.permute)\n",
    "    \n",
    "    \n",
    "class BatchNormFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim, momentum=0.95, eps=1e-5):\n",
    "        super(BatchNormFlow, self).__init__()\n",
    "        # Running batch statistics\n",
    "        self.r_mean = torch.zeros(dim)\n",
    "        self.r_var = torch.ones(dim)\n",
    "        # Momentum\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        # Trainable scale and shift (cf. original paper)\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "    def _call(self, z):\n",
    "        if self.training:\n",
    "            # Current batch stats\n",
    "            self.b_mean = z.mean(0)\n",
    "            self.b_var = (z - self.b_mean).pow(2).mean(0) + self.eps\n",
    "            # Running mean and var\n",
    "            self.r_mean = self.momentum * self.r_mean + ((1 - self.momentum) * self.b_mean)\n",
    "            self.r_var = self.momentum * self.r_var + ((1 - self.momentum) * self.b_var)\n",
    "            mean = self.b_mean\n",
    "            var = self.b_var\n",
    "        else:\n",
    "            mean = self.r_mean\n",
    "            var = self.r_var\n",
    "        x_hat = (z - mean) / var.sqrt()\n",
    "        y = self.gamma * x_hat + self.beta\n",
    "        return y\n",
    "\n",
    "    def _inverse(self, x):\n",
    "        if self.training:\n",
    "            mean = self.b_mean\n",
    "            var = self.b_var\n",
    "        else:\n",
    "            mean = self.r_mean\n",
    "            var = self.r_var\n",
    "        x_hat = (z - self.beta) / self.gamma\n",
    "        y = x_hat * var.sqrt() + mean\n",
    "        return y\n",
    "        \n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        # Here we only need the variance\n",
    "        mean = z.mean(0)\n",
    "        var = (z - mean).pow(2).mean(0) + self.eps\n",
    "        log_det = torch.log(self.gamma) - 0.5 * torch.log(var + self.eps)\n",
    "        return torch.sum(log_det, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 11-dimensional Rosenbrock Distribution Data\n",
    "\n",
    "We use planar flow as above to correct the severely truncated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 11d Rosen sample data\n",
    "data2 = scipy.io.loadmat(\"./data/tt_irt_rosen_sample.mat\")\n",
    "data2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data2['training_data'] # severely truncated TT samples, used as training data to flow in every epoch\n",
    "# get log approximate sample densities from TT-IRT\n",
    "log_training_data_densities = data2['training_data_densities']\n",
    "#training_data_densities = np.exp(log_training_data_densities)\n",
    "#training_data_densities_normalized = training_data_densities / training_data_densities.sum()\n",
    "#log_training_data_densities = np.log(training_data_densities)\n",
    "\n",
    "\n",
    "\n",
    "# data used for training\n",
    "N = data2['N'][0][0]\n",
    "num_epoch = data2['epoch'][0][0]\n",
    "rosen_dim = data2['d'][0][0]\n",
    "# normalization constant\n",
    "rosen_norm = data2['rosen_norm_const'][0][0]\n",
    "print(\">>> shape of all training data: \", training_data.shape)\n",
    "\n",
    "\n",
    "#print(\">>> shape of sample data from untruncated TT: \", tt_sample_trunc.shape)\n",
    "#print(\">>> shape of sample data from truncated TT: \", tt_sample_trunc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at heavy tail\n",
    "np.random.seed(10)\n",
    "data_idx = np.random.randint(0, training_data.shape[0], N)\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta_{}$\".format(rosen_dim-1)); \n",
    "plt.ylabel(\"$\\\\theta_{}$\".format(rosen_dim)); \n",
    "plt.title(\"Samples from Truncated TT\"); \n",
    "plot_data_trunc = training_data[data_idx, :]\n",
    "plt.scatter(plot_data_trunc[:,rosen_dim-2], plot_data_trunc[:,rosen_dim-1], color='blue', s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin correction\n",
    "\n",
    "# define Rosenbrock function for density evaluation\n",
    "def rosen(theta):\n",
    "    \"\"\" theta is a torch tensor in R^(Nxd). \"\"\"\n",
    "    d = np.shape(theta)[1]\n",
    "    # formula from paper: An n-dimensional Rosenbrock Distribution for MCMC Testing\n",
    "    # unknown normalization constant\n",
    "    result = 0\n",
    "    for k in range(d-1):\n",
    "        result += (theta[:,k])**2 + ( theta[:,k+1] + 5 * ((theta[:,k])**2 + 1) )**2\n",
    "    return torch.exp(-0.5*result) / rosen_norm\n",
    "\n",
    "# need to redefine loss (takes in a PDF function instead of a pytorch distribution object)\n",
    "def loss2(density, zk, log_jacobians):\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    # free energy lower bound [Variational Inference with Normalizing Flows]\n",
    "    return (-sum_of_log_jacobians - torch.log(density(zk)+1e-10)).mean()\n",
    "\n",
    "def loss_kl(prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" prior_distrb, targ_distrib need to be PyTorch distribution objects. \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    # ELBO\n",
    "    return (prior_distrib.log_prob(z0) - sum_of_log_jacobians - targ_distrib.log_prob(zk)).mean()\n",
    "def loss_kl_2(prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" sae as LOSS_KL() but targ_distrib is callable, rather than PyTorch distribution. \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (prior_distrib.log_prob(z0) - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()\n",
    "\n",
    "def loss_kl_3(log_prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" sae as LOSS_KL() but:\n",
    "        - log_prior_distrib is predetermined as passed in as a torch.Tensor\n",
    "        - targ_distrib is callable, rather than PyTorch distribution. \n",
    "    \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (log_prior_distrib - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin training\n",
    "flow_type = \"planar\" # autoregressive\n",
    "if flow_type == \"planar\":\n",
    "    rosen_planar_flow = NormalizingFlow(dim=rosen_dim, blocks=None, flow_length=32, density=None)\n",
    "elif flow_type == \"autoregressive\":\n",
    "    block_real_nvp = [ AffineCouplingFlow, ReverseFlow, BatchNormFlow ]\n",
    "    rosen_planar_flow = NormalizingFlow(dim=rosen_dim, blocks=block_real_nvp, flow_length=12, \\\n",
    "                                        density=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer with adaptive learning rate\n",
    "import torch.optim as optim\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(rosen_planar_flow.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin training! we use the sample from severely truncated TT and \n",
    "# the flow\n",
    "#          x_TT = f(x_TT_truncated)\n",
    "# train the inverse flow, i.e.\n",
    "#          x_TT_truncated = g(x_TT) = f^{-1}(x_TT)\n",
    "\n",
    "# check exists\n",
    "training_data; N; data_idx; rosen_dim;\n",
    "save_training_data = training_data\n",
    "\n",
    "# As per discussion with Michael, shift the Gaussian by estimated mean \n",
    "# and scale by estimated std in each dimension\n",
    "est_mean = training_data.mean(0)\n",
    "est_std = training_data.std(0)\n",
    "\n",
    "# d-DIMENSIONAL standard normal as toy training base\n",
    "ref_distrib = distrib.MultivariateNormal(torch.Tensor(est_mean), torch.diag(torch.Tensor(est_std)))\n",
    "targ_distrib = distrib.MultivariateNormal(torch.zeros([rosen_dim]), torch.eye(rosen_dim))\n",
    "\n",
    "\n",
    "use_normal = False # use multivariate normal as base distribution\n",
    "if use_normal:\n",
    "    training_data = ref_distrib.sample((training_data.shape[0], ))\n",
    "else:\n",
    "    training_data = save_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample from training data\n",
    "data_idx;\n",
    "plot_data = training_data[data_idx, :]\n",
    "plt.figure(figsize=(8,6)); plt.grid(True); plt.xlabel(\"dim 10\"); plt.ylabel(\"dim 11\");\n",
    "plt.title(\"base distribution\");\n",
    "plt.scatter(plot_data[:,rosen_dim-2], plot_data[:,rosen_dim-1], color=\"blue\", s=2);\n",
    "\n",
    "# plot a sample from standard normal distribution\n",
    "data_idx;\n",
    "plot_data = targ_distrib.sample((len(data_idx), ))\n",
    "plt.figure(figsize=(8,6)); plt.grid(True); plt.xlabel(\"dim 10\"); plt.ylabel(\"dim 11\");\n",
    "plt.title(\"standard normal distribution\");\n",
    "plt.scatter(plot_data[:,rosen_dim-2], plot_data[:,rosen_dim-1], color=\"red\", s=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimization loop\n",
    "num_iter = 10000\n",
    "num_subplot = 0\n",
    "all_losses = []\n",
    "\n",
    "# define loss function we are using\n",
    "criterion = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n",
    "for it in range(num_iter+1):\n",
    "    # Draw a random sample batch from \"base\" (the truncated TT samples, in training data)\n",
    "    # row indices\n",
    "    sample_idx = np.random.choice(training_data.shape[0], N)\n",
    "    samples = torch.Tensor(training_data[sample_idx,:])\n",
    "    #samples = torch.Tensor(training_data)\n",
    "    \n",
    "    # draw from normal and see if it captures Rosenbrock\n",
    "    #samples = ref_distrib.sample((N, ))\n",
    "    # flow this sample\n",
    "    zk, log_jacobians = rosen_planar_flow(samples)\n",
    "    \n",
    "    if use_normal:\n",
    "        # if we are using normal, the base density is simple to evaluate\n",
    "        log_base_prob = ref_distrib.log_prob(samples).reshape(-1, 1)\n",
    "        # volume correction (approximate dist.)\n",
    "        log_base_prob -= sum(log_jacobians).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "        # target density\n",
    "        #targ_prob = rosen(torch.Tensor(zk))  \n",
    "        targ_prob = targ_distrib.log_prob(torch.Tensor(zk))\n",
    "    else:\n",
    "        # need evaluating TT density\n",
    "        log_base_prob = torch.Tensor(log_training_data_densities[sample_idx, :]).reshape(-1,1)\n",
    "        \n",
    "        # volume correction (approximate dist.)\n",
    "        #log_base_prob -= sum(log_jacobians).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "        # target density\n",
    "        #targ_prob = rosen(torch.Tensor(zk))\n",
    "        targ_prob = targ_distrib.log_prob(torch.Tensor(zk))\n",
    "        \n",
    "        base_prob = log_base_prob\n",
    "        \n",
    "    ### visual reporting\n",
    "    if (it % 1000 == 0):\n",
    "        #print(rosen_planar_flow.weight)\n",
    "        # plot the flowed samples\n",
    "        num_subplot += 1\n",
    "        plt.figure(2, figsize=(100,80));\n",
    "        plt.subplot(6,6,num_subplot); plt.grid(True); \n",
    "        plt.xlabel(\"$dim {}$\".format(rosen_dim-1)); \n",
    "        plt.ylabel(\"$dim {}$\".format(rosen_dim)); \n",
    "        plt.title(\"{} at iter. {}\".format(num_subplot, it));\n",
    "        # pick a sample from training data, flow, and plot it\n",
    "        data_idx = np.random.randint(0, training_data.shape[0], N)\n",
    "        plot_data = torch.Tensor(training_data[data_idx, :])\n",
    "        flowed_plot_data, _ = rosen_planar_flow(plot_data)\n",
    "        flowed_plot_data = flowed_plot_data.detach().numpy()\n",
    "        plt.scatter(flowed_plot_data[:,rosen_dim-2], flowed_plot_data[:,rosen_dim-1], color='purple', s=5);\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # compute loss of on the flowed sample\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss2(rosen, zk, log_jacobians)\n",
    "    \n",
    "    # <does not work>\n",
    "    #loss_v = criterion(log_base_prob, targ_prob)\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss_kl(ref_distrib, targ_distrib, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, but learns wrong distribution)\n",
    "    #loss_v = loss_kl_2(ref_distrib, rosen, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, ideal solution>\n",
    "    loss_v = loss_kl_3(base_prob, rosen, samples, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 500 == 0):\n",
    "        #print('Log ML Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        print('KL Divergence Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # for plotting loss\n",
    "        all_losses.append(loss_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.grid(True);\n",
    "plt.title(\"ELBO vs. Training Loop\");\n",
    "plt.plot(500*np.array(range(len(all_losses))), all_losses, color='green', marker='o', label='loss');\n",
    "plt.xlabel(\"num iter. \"); plt.ylabel('ELBO');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(all_losses, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot flowed result after training\n",
    "\n",
    "# flow all truncated data\n",
    "plt.figure(1, figsize=(20,8)); \n",
    "plt.subplot(1,2,1);\n",
    "plt.grid(True);\n",
    "plt.title(\"Heavy Tail Samples Before Flow Correction (KL Div.), Num Samples = {}\".format(N)); \n",
    "plt.xlabel(\"$\\\\theta_{}$\".format(rosen_dim-1)); plt.ylabel(\"$\\\\theta_{}$\".format(rosen_dim));\n",
    "# scatter the heavy tail part\n",
    "plt.scatter(training_data[data_idx,:][:,rosen_dim-2], training_data[data_idx,:][:,rosen_dim-1], s=1, color='green');\n",
    "\n",
    "plt.subplot(1,2,2);\n",
    "plt.grid(True);\n",
    "plt.title(\"Heavy Tail Samples After Flow Correction (KL Div.), Num Samples = {}\".format(N)); \n",
    "plt.xlabel(\"$\\\\theta_{}$\".format(rosen_dim-1)); plt.ylabel(\"$\\\\theta_{}$\".format(rosen_dim));\n",
    "# use NF to flow first\n",
    "# row indices\n",
    "#sample_idx = np.random.randint(0, training_data.shape[0], N)\n",
    "samples = torch.Tensor(training_data[sample_idx,:])\n",
    "#samples = ref_distrib.sample((N, ))\n",
    "\n",
    "zk, _ = rosen_planar_flow(samples)\n",
    "\n",
    "# plot\n",
    "zk = zk.detach().numpy()\n",
    "plt.scatter(zk[:,rosen_dim-2], zk[:,rosen_dim-1], s=1, color='blue');\n",
    "print(zk.mean(0))\n",
    "print(zk.std(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 Dimensional Transition State Governed by Ginzburg-Landau Energy in 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 50d Rosen sample data\n",
    "data_ginz = scipy.io.loadmat(\"./data/tt_irt_ginzburg1d_sample.mat\")\n",
    "display(data_ginz.keys())\n",
    "# very high precision\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data from truncated TT\n",
    "training_data = data_ginz['training_data_ginsburg']\n",
    "log_training_data_densities = data_ginz['training_densities_ginsburg']\n",
    "# used for comparison, from accurate TT\n",
    "comparison_data = data_ginz['comparison_data_ginsburg']\n",
    "comparison_data_densities = data_ginz['comparison_densities_ginsburg']\n",
    "print(\"====== shape of training data = {}\".format(training_data.shape))\n",
    "num_samples = data_ginz['N'][0][0]\n",
    "num_epoch = data_ginz['epoch'][0][0]\n",
    "# seed used to generate samples\n",
    "unif_seed = data_ginz['unif_sample']\n",
    "# dimension of the PDF\n",
    "ginz_dim = unif_seed.shape[1]\n",
    "# normalization constant\n",
    "Z_beta = data_ginz['Z_beta'][0][0]\n",
    "# parameters\n",
    "temperature = data_ginz['temp'][0][0]\n",
    "ginz_delta = data_ginz['delta'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def ginzburg_landau_energy1d(U, delta=ginz_delta):\n",
    "    \"\"\" computes the GL energy (1d) for U, U is an torch.Tensor of shape N x d. \n",
    "    \n",
    "    Outputs energy as shape (N x 1) tensor\n",
    "    \n",
    "    \"\"\"\n",
    "    # make row vector\n",
    "    U = torch.Tensor(U)\n",
    "    N = U.shape[0]\n",
    "    d = U.shape[1]\n",
    "    # compute stepsize\n",
    "    h = 1 / (d+1)\n",
    "    # compute energy with U_0 and U_d+1\n",
    "    # pad zeros\n",
    "    #U = torch.cat((torch.zeros([N, 1]), U, torch.zeros([N, 1])), dim=1)\n",
    "    \n",
    "    \n",
    "    V = ( (delta/2) * ( ((1/h) * (U[:,1:d] - U[:,0:d-1]))**2 ) + \\\n",
    "    (1/(4 * delta)) * ( (1 - U[:,1:d]**2 )**2 ) ).sum(1)\n",
    "    return V\n",
    "\n",
    "def equilibrium_pdf(U, delta=ginz_delta, beta=1/temperature, normalization_const=Z_beta):\n",
    "    V = ginzburg_landau_energy1d(U, delta)\n",
    "    prob = ( 1/normalization_const ) * torch.exp(-beta * V)\n",
    "    return prob\n",
    "\n",
    "print(\"===== testing ginzburg_landau_energy1d\\n\")\n",
    "print(ginzburg_landau_energy1d(torch.rand([10,50])))\n",
    "\n",
    "print(\"===== testing equilibrium probability density\\n\")\n",
    "print(equilibrium_pdf(torch.rand([10,50])))\n",
    "\n",
    "def loss_kl_3(log_prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" same as LOSS_KL() but:\n",
    "        - log_prior_distrib is predetermined as passed in as a torch.Tensor\n",
    "        - targ_distrib is callable, rather than PyTorch distribution. \n",
    "    \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (log_prior_distrib - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()\n",
    "\n",
    "def loss_kl_2(prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" sae as LOSS_KL() but targ_distrib is callable, rather than PyTorch distribution. \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (prior_distrib.log_prob(z0) - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "data_idx = np.random.randint(0, training_data.shape[0], 100*num_samples)\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta {}$\".format(ginz_dim-1)); \n",
    "plt.ylabel(\"$\\\\theta {}$\".format(ginz_dim)); \n",
    "plt.title(\"Samples from Truncated TT\"); \n",
    "plot_data_trunc = training_data[data_idx, :]\n",
    "# it should look like two modes\n",
    "plt.hexbin(plot_data_trunc[:,ginz_dim-2], plot_data_trunc[:,ginz_dim-1]);\n",
    "\n",
    "# comparison with accurate TT samples\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta {}$\".format(ginz_dim-1)); \n",
    "plt.ylabel(\"$\\\\theta {}$\".format(ginz_dim)); \n",
    "plt.title(\"Samples from Accurate TT\"); \n",
    "plot_data_trunc = comparison_data[data_idx, :]\n",
    "# it should look like two modes\n",
    "plt.hexbin(plot_data_trunc[:,ginz_dim-2], plot_data_trunc[:,ginz_dim-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin training \n",
    "flow_type = 'planar_flow'\n",
    "if flow_type == 'planar_flow':\n",
    "    ginz_flow = NormalizingFlow(dim=ginz_dim, blocks=None, flow_length=32, density=None)\n",
    "elif flow_type == 'autoregressive':\n",
    "    raise NotImplementedError(\"Autoregressive Flow needs to be implemented. \")\n",
    "    \n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(ginz_flow.parameters(), lr=1e-2)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimization loop\n",
    "num_iter = 10000\n",
    "num_subplot = 0\n",
    "all_losses = []\n",
    "batch_size = 2**12\n",
    "\n",
    "\n",
    "# check exists\n",
    "training_data; ginz_dim;\n",
    "save_training_data = training_data\n",
    "\n",
    "# As per discussion with Michael, shift the Gaussian by estimated mean \n",
    "# and scale by estimated std in each dimension\n",
    "est_mean = training_data.mean(0)\n",
    "est_std = training_data.std(0)\n",
    "\n",
    "# d-DIMENSIONAL standard normal as toy training base\n",
    "ref_distrib = distrib.MultivariateNormal(torch.Tensor(est_mean), torch.diag(torch.Tensor(est_std)))\n",
    "targ_distrib = distrib.MultivariateNormal(torch.zeros([ginz_dim]), torch.eye(ginz_dim))\n",
    "\n",
    "\n",
    "use_normal = False # use multivariate normal as base distribution\n",
    "if use_normal:\n",
    "    training_data = ref_distrib.sample((training_data.shape[0], ))\n",
    "else:\n",
    "    training_data = save_training_data\n",
    "\n",
    "# define loss function we are using\n",
    "#criterion = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n",
    "for it in range(num_iter+1):\n",
    "    # Draw a random sample batch from \"base\" (the truncated TT samples, in training data)\n",
    "    # row indices\n",
    "    sample_idx = np.random.choice(training_data.shape[0], batch_size)\n",
    "    samples = torch.Tensor(training_data[sample_idx,:])\n",
    "    #samples = torch.Tensor(training_data)\n",
    "    \n",
    "    # draw from normal and see if it captures Rosenbrock\n",
    "    #samples = ref_distrib.sample((N, ))\n",
    "    # flow this sample\n",
    "    zk, log_jacobians = ginz_flow(samples)\n",
    "    \n",
    "    if use_normal:\n",
    "        # if we are using normal, the base density is simple to evaluate\n",
    "        log_base_prob = ref_distrib.log_prob(samples).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "\n",
    "    else:\n",
    "        # need evaluating TT density\n",
    "        log_base_prob = torch.Tensor(log_training_data_densities[sample_idx, :]).reshape(-1,1)\n",
    "        \n",
    "        # volume correction (approximate dist.)\n",
    "        #log_base_prob -= sum(log_jacobians).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "        base_prob = log_base_prob\n",
    "        \n",
    "    ### visual reporting\n",
    "    if (it % 1000 == 0):\n",
    "        # plot the flowed samples\n",
    "        num_subplot += 1\n",
    "        plt.figure(2, figsize=(8,6));\n",
    "        plt.grid(True); \n",
    "        plt.xlabel(\"$dim {}$\".format(ginz_dim-1)); \n",
    "        plt.ylabel(\"$dim {}$\".format(ginz_dim)); \n",
    "        plt.title(\"{} at iter. {}\".format(num_subplot, it));\n",
    "        # pick a sample from training data, flow, and plot it\n",
    "        data_idx = np.random.randint(0, training_data.shape[0], 10*num_samples)\n",
    "        plot_data = torch.Tensor(training_data[data_idx, :])\n",
    "        flowed_plot_data, _ = ginz_flow(plot_data)\n",
    "        flowed_plot_data = flowed_plot_data.detach().numpy()\n",
    "        plt.hexbin(flowed_plot_data[:,ginz_dim-2], flowed_plot_data[:,ginz_dim-1]);\n",
    "        # save figure\n",
    "        plt.savefig(\"./img/GL_1d/ginzburg1d_batchsz{}_train_iter{}\".format(batch_size, it))\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # compute loss of on the flowed sample\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss2(rosen, zk, log_jacobians)\n",
    "    \n",
    "    # <does not work>\n",
    "    #loss_v = criterion(log_base_prob, targ_prob)\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss_kl(ref_distrib, targ_distrib, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, but learns wrong distribution)\n",
    "    #loss_v = loss_kl_2(ref_distrib, equilibrium_pdf, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, ideal solution>\n",
    "    loss_v = loss_kl_3(base_prob, equilibrium_pdf, samples, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 500 == 0):\n",
    "        #print('Log ML Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        print('KL Divergence Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # for plotting loss\n",
    "        all_losses.append(loss_v.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 Dimensional Transition State Governed by Ginzburg-Landau Energy in 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 100d GL sample data\n",
    "data_ginz2d = scipy.io.loadmat(\"./data/tt_irt_ginzburg2d_sample.mat\")\n",
    "display(data_ginz2d.keys())\n",
    "# very high precision\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data from truncated TT\n",
    "training_data = data_ginz2d['training_data']\n",
    "log_training_data_densities = data_ginz2d['training_densities']\n",
    "# used for comparison, from accurate TT\n",
    "comparison_data = data_ginz2d['gl_TT_samples']\n",
    "\n",
    "print(\"====== shape of training data = {}\".format(training_data.shape))\n",
    "num_samples = data_ginz2d['N'][0][0]\n",
    "N = int(num_samples / 10)\n",
    "# seed used to generate samples\n",
    "unif_seed = data_ginz2d['unif_sample']\n",
    "# dimension of the PDF\n",
    "ginz_dim = unif_seed.shape[1]\n",
    "# normalization constant\n",
    "Z_beta = data_ginz2d['Z_beta'][0][0]\n",
    "# parameters\n",
    "temperature = data_ginz2d['temp'][0][0]\n",
    "ginz_delta = data_ginz2d['delta'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def ginzburg_landau_energy2d(U, delta=ginz_delta):\n",
    "    \"\"\" computes the GL energy (1d) for U, U is an torch.Tensor of shape N x d. \n",
    "    \n",
    "    Outputs energy as shape (N x 1) tensor\n",
    "    \n",
    "    \"\"\"\n",
    "    # make row vector\n",
    "    U = torch.Tensor(U)\n",
    "    N = U.shape[0]\n",
    "    d = U.shape[1]\n",
    "    # dimension must be a perfect square\n",
    "    board_dim = int(math.sqrt(d) + 0.5) # spatial dimension, instead of the PDF dimension\n",
    "    assert board_dim**2 == d, \"dimension is not a perfect square\"\n",
    "    # compute stepsize\n",
    "    h = 1 / (board_dim+1)\n",
    "    # torch reshape is different from MATLAB reshape\n",
    "    # MATLAB is by column, torch is by row\n",
    "    # we use MATLAB's format here, so need to transpose the result of torch.reshape\n",
    "    U_2d = U.reshape([N, board_dim, board_dim])\n",
    "    U_2d = torch.transpose(U_2d, 1, 2)\n",
    "    # add boundary values\n",
    "    U_2d_save = U_2d.clone()\n",
    "    U_2d = torch.zeros([N, board_dim+2, board_dim+2]) # including U0, Ud+1\n",
    "    U_2d[:,1:board_dim+1,1:board_dim+1] = U_2d_save\n",
    "\n",
    "    # compute energy with U_0 and U_d+1\n",
    "    u_x = (1 / h) * (U_2d[:, 1:board_dim+2, :] - U_2d[:, 0:board_dim+1, :])\n",
    "    u_y = (1 / h) * (U_2d[:, :, 1:board_dim+2] - U_2d[:, :, 0:board_dim+1])\n",
    "    V = (delta/2)*( (u_x**2).sum([1,2]) ) + \\\n",
    "     (delta/2) * ( (u_y**2).sum([1,2]) ) + \\\n",
    "    (1/(4*delta)) * ((1 - U_2d ** 2) ** 2).sum([1,2])\n",
    "    return V\n",
    "\n",
    "def equilibrium_pdf(U, delta=ginz_delta, beta=1/temperature, normalization_const=Z_beta):\n",
    "    V = ginzburg_landau_energy2d(U, delta)\n",
    "    prob = ( 1/normalization_const ) * torch.exp(-beta * V)\n",
    "    return prob\n",
    "\n",
    "print(\"===== testing ginzburg_landau_energy2d\\n\")\n",
    "print(ginzburg_landau_energy2d(torch.rand([2**15,100])))\n",
    "\n",
    "print(\"===== testing equilibrium probability density\\n\")\n",
    "print(equilibrium_pdf(torch.rand([2**15,100])))\n",
    "\n",
    "def loss_kl_3(log_prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" same as LOSS_KL() but:\n",
    "        - log_prior_distrib is predetermined as passed in as a torch.Tensor\n",
    "        - targ_distrib is callable, rather than PyTorch distribution. \n",
    "    \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (log_prior_distrib - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()\n",
    "\n",
    "def loss_kl_2(prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" sae as LOSS_KL() but targ_distrib is callable, rather than PyTorch distribution. \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (prior_distrib.log_prob(z0) - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ginzburg_landau_energy2d(2*torch.ones([1, 36]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "which = 3 # which plane are we plotting, plot(dim(D-which-1),dim(D-which))\n",
    "plot_samples = num_samples * 4\n",
    "data_idx = np.random.randint(0, training_data.shape[0], plot_samples)\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta {}$\".format(ginz_dim-which)); \n",
    "plt.ylabel(\"$\\\\theta {}$\".format(ginz_dim-which+1)); \n",
    "plt.title(\"Samples from Truncated TT\"); \n",
    "plot_data_trunc = training_data[data_idx, :]\n",
    "\n",
    "plt.hexbin(plot_data_trunc[:,ginz_dim-which-1], plot_data_trunc[:,ginz_dim-which]);\n",
    "\n",
    "# comparison with accurate TT samples\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta {}$\".format(ginz_dim-which)); \n",
    "plt.ylabel(\"$\\\\theta {}$\".format(ginz_dim-which+1)); \n",
    "plt.title(\"Samples from Accurate TT\"); \n",
    "plot_data_trunc = comparison_data[data_idx, :]\n",
    "# it should look like two modes\n",
    "plt.hexbin(plot_data_trunc[:,ginz_dim-which-1], plot_data_trunc[:,ginz_dim-which]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin training \n",
    "flow_type = 'planar_flow'\n",
    "if flow_type == 'planar_flow':\n",
    "    ginz_flow = NormalizingFlow(dim=ginz_dim, blocks=None, flow_length=32, density=None)\n",
    "elif flow_type == 'autoregressive':\n",
    "    raise NotImplementedError(\"Autoregressive Flow needs to be implemented. \")\n",
    "    \n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(ginz_flow.parameters(), lr=1e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimization loop\n",
    "num_iter = 10000\n",
    "num_subplot = 0\n",
    "all_losses = []\n",
    "batch_size = 2**12\n",
    "\n",
    "\n",
    "# check exists\n",
    "training_data; ginz_dim;\n",
    "save_training_data = training_data\n",
    "\n",
    "# As per discussion with Michael, shift the Gaussian by estimated mean \n",
    "# and scale by estimated std in each dimension\n",
    "est_mean = training_data.mean(0)\n",
    "est_std = training_data.std(0)\n",
    "\n",
    "# d-DIMENSIONAL standard normal as toy training base\n",
    "ref_distrib = distrib.MultivariateNormal(torch.Tensor(est_mean), torch.diag(torch.Tensor(est_std)))\n",
    "targ_distrib = distrib.MultivariateNormal(torch.zeros([ginz_dim]), torch.eye(ginz_dim))\n",
    "\n",
    "\n",
    "use_normal = False # use multivariate normal as base distribution\n",
    "if use_normal:\n",
    "    training_data = ref_distrib.sample((training_data.shape[0], ))\n",
    "else:\n",
    "    training_data = save_training_data\n",
    "\n",
    "# define loss function we are using\n",
    "#criterion = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n",
    "for it in range(num_iter+1):\n",
    "    # Draw a random sample batch from \"base\" (the truncated TT samples, in training data)\n",
    "    # row indices\n",
    "    sample_idx = np.random.choice(training_data.shape[0], batch_size)\n",
    "    samples = torch.Tensor(training_data[sample_idx,:])\n",
    "    #samples = torch.Tensor(training_data)\n",
    "    \n",
    "    # draw from normal and see if it captures Rosenbrock\n",
    "    #samples = ref_distrib.sample((N, ))\n",
    "    # flow this sample\n",
    "    zk, log_jacobians = ginz_flow(samples)\n",
    "    \n",
    "    if use_normal:\n",
    "        # if we are using normal, the base density is simple to evaluate\n",
    "        log_base_prob = ref_distrib.log_prob(samples).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "\n",
    "    else:\n",
    "        # need evaluating TT density\n",
    "        log_base_prob = torch.Tensor(log_training_data_densities[sample_idx, :]).reshape(-1,1)\n",
    "        \n",
    "        # volume correction (approximate dist.)\n",
    "        #log_base_prob -= sum(log_jacobians).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "        base_prob = log_base_prob\n",
    "        \n",
    "    ### visual reporting\n",
    "    if (it % 1000 == 0):\n",
    "        #print(rosen_planar_flow.weight)\n",
    "        # plot the flowed samples\n",
    "        num_subplot += 1\n",
    "        plt.figure(2, figsize=(8,6));\n",
    "        plt.grid(True); \n",
    "        plt.xlabel(\"$dim {}$\".format(ginz_dim-1)); \n",
    "        plt.ylabel(\"$dim {}$\".format(ginz_dim)); \n",
    "        plt.title(\"{} at iter. {}\".format(num_subplot, it));\n",
    "        # pick a sample from training data, flow, and plot it\n",
    "        data_idx = np.random.randint(0, training_data.shape[0], 10*num_samples)\n",
    "        plot_data = torch.Tensor(training_data[data_idx, :])\n",
    "        flowed_plot_data, _ = ginz_flow(plot_data)\n",
    "        flowed_plot_data = flowed_plot_data.detach().numpy()\n",
    "        plt.hexbin(flowed_plot_data[:,ginz_dim-2], flowed_plot_data[:,ginz_dim-1]);\n",
    "        # save figure\n",
    "        plt.savefig(\"./img/GL_2d/ginzburg2d_batchsz{}_train_iter{}\".format(batch_size, it))\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # compute loss of on the flowed sample\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss2(rosen, zk, log_jacobians)\n",
    "    \n",
    "    # <does not work>\n",
    "    #loss_v = criterion(log_base_prob, targ_prob)\n",
    "    \n",
    "    # <works>\n",
    "    #loss_v = loss_kl(ref_distrib, targ_distrib, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, but learns wrong distribution)\n",
    "    #loss_v = loss_kl_2(ref_distrib, equilibrium_pdf, samples, zk, log_jacobians)\n",
    "    \n",
    "    # <works, ideal solution>\n",
    "    loss_v = loss_kl_3(base_prob, equilibrium_pdf, samples, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 500 == 0):\n",
    "        #print('Log ML Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        print('KL Divergence Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # for plotting loss\n",
    "        all_losses.append(loss_v.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonequilibrium Path Sampling Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path space is governed by the following SDE:\n",
    "$$\n",
    "    dX_t = b(X_t)dt + \\sqrt{2\\beta^{-1}}dW_t\n",
    "$$ where $X_t\\in\\mathbb{R}^2$, drift $b$ is non-conservative, $\\beta \\propto \\frac{1}{T}$ is inverse temperature of the system. With end points conditioning in the path sapce, we may define a path-integral type distribution (formal):\n",
    "$$\n",
    "    P_{*}(x_{[0,t_m]}) \\propto \\exp(-\\frac{\\beta}{4}\\int_{0}^{t_m} |\\dot{x_t} - b(x_t) |^2 dt)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end point conditions are $x_0 = x_A = [-1,0]$, $x_{t_m} = x_B = [1,0]$. Furthermore, we need to discretize the integral using equidistant time steps $t_0, t_1, \\cdots, t_N=t_m$, and a step size $\\Delta t$, which yields the target distribution:\n",
    "$$\n",
    "    \\tilde{P}_{*}(x_1,x_2,\\cdots, x_{N-1}; x_0=x_A,x_N=x_B) \\propto \\exp\\bigg[\n",
    "        -\\frac14 S_*(x_1,x_2,\\cdots, x_{N-1};x_0,x_N)\n",
    "    \\bigg]\n",
    "$$ where $S_*$ denotes the target path action:\n",
    "$$\n",
    "    S_*(x_1,\\cdots, x_{N-1}; x_0, x_N) = \\beta \\Delta t \\bigg[\\big(\\sum_{i=1}^{N-1}| \\frac{x_{i+1} - x_{i}}{\\Delta t} - b(x_i)|^2\\big) + |\\frac{x_1 - x_A}{\\Delta t} - b(x_1)|^2 + |\\frac{x_B - x_{N-1}}{\\Delta t} - b(x_{N-1})|^2 \\bigg]\n",
    "$$ where the last two terms are \"penalties\" for boundary points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-conservative drift term is given by $b: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$:\n",
    "$$\n",
    "    b(x) = -\\nabla V(x) + f(x)\n",
    "$$ where:\n",
    "\n",
    "$$\n",
    "     V(x) = A_1 \\exp({|{x-\\mu_1}|^2}) + A_2 \\exp({|{x-\\mu_2}|^2}) + A_3 \\exp({|{x-\\mu_3}|^2}) + A_4 \\exp({|{x-\\mu_4}|^2}) + A_5\\big[(x^{(1)} - \\mu_1^{(1)})^4 + (x^{(2)} - \\mu_1^{(2)})^4\\big]\n",
    "$$  \n",
    "\n",
    "\n",
    "$V$ is a weighted sum of Gaussians that define our three-hole potential [[Metzner]](https://aip-scitation-org.proxy.uchicago.edu/doi/10.1063/1.2335447) where the last two quartic terms are to prevent exponential decay (and thus exponential growth of probability mass) on the boundary.\n",
    "\n",
    "$f(x) = (-x^{(2)}, x^{(1)})^T$ is the nonconservative term that adds a counterclockwise spin (by $\\pi/2$, consider mapping $f: (\\sin z, \\cos z)\\mapsto (-\\cos z, \\sin z)$) to the path."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments:\n",
    "$$\n",
    "    A_1 = 40; A_2 = -55; A_3 = -50; A_4 = -50; A_5 = 0.2;\n",
    "$$\n",
    "$$\n",
    "    \\beta = 1, N = ??\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 100d GL sample data\n",
    "data_transition = scipy.io.loadmat(\"./data/tt_irt_path_sampling.mat\")\n",
    "display(data_transition.keys())\n",
    "# very high precision\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data from truncated TT\n",
    "training_data = data_transition['path_samples']\n",
    "log_training_data_densities = np.log(data_transition['path_sample_densities']).reshape(-1)\n",
    "\n",
    "print(\"====== shape of training data = {}\".format(training_data.shape))\n",
    "num_samples = data_transition['M'][0][0]\n",
    "# seed used to generate samples\n",
    "unif_seed = data_transition['unif_sample']\n",
    "# dimension of the PDF\n",
    "transition_dim = data_transition['N'][0][0] * 2\n",
    "\n",
    "# parameters\n",
    "path_beta = data_transition['beta'][0][0]\n",
    "path_dt = data_transition['dt'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visialize paths (uncorrected)\n",
    "path_sampels_uncorrected = training_data.reshape([num_samples, 2, transition_dim // 2]).transpose(0, 2, 1)\n",
    "for i in range(num_samples):\n",
    "    plt.plot(path_sampels_uncorrected[i,:,0], path_sampels_uncorrected[i,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def path_action(U, beta=path_beta, dt=path_dt):\n",
    "    \"\"\" computes the effective energy (path integral) for path sample U. \n",
    "    \n",
    "    Inputs:\n",
    "        U,                          (torch.Tensor) (M x 2*N) path samples, each row is a path\n",
    "                                    with time discretization level N\n",
    "        beta,                       (scalar)       inverse temperature, parameter\n",
    "        dt,                         (scalar)       time discretization level, parameter\n",
    "        \n",
    "    Outputs:\n",
    "        S,                          (torch.Tensor) (M x 1)   energy for each path of U\n",
    "    \"\"\"\n",
    "    U = torch.Tensor(U) # make sure of formatting\n",
    "    assert U.shape[1] % 2 == 0, \"Dimension of path must be divisible by 2. \"\n",
    "    assert U.requires_grad, \"U must require grad, for auto-differentiation. \"\n",
    "    xA = torch.tensor([[-1., 0.]]); xB = torch.tensor([[1., 0.]])\n",
    "    M = int(U.shape[0])\n",
    "    N = int(U.shape[1] // 2)\n",
    "    # reshape to (M x N x 2), torch reshapes by row, hence the transpose\n",
    "    U_2d = U.reshape([M, 2, N])\n",
    "    U_2d = U_2d.transpose(1, 2) # U has size (M x N x 2)\n",
    "    U_2d = U_2d.view(U_2d.shape[0], -1, 2)\n",
    "    # get gradient of potential, has size (M x N x 2) the same as U_2d\n",
    "    grad_V = torch.autograd.grad(V(U_2d).sum(), U_2d, create_graph=True)[0]\n",
    "    # compute path integral with differencing\n",
    "    S = torch.sum( ( (U_2d[:,1:,:] - U_2d[:,:-1,:]) / dt + grad_V[:,:-1,:] )**2, dim=(1, 2))\n",
    "    # penalize boundary xA\n",
    "    S += torch.sum(\n",
    "            ( (U_2d[:,0,:] - xA) / dt + grad_V[:,0,:] )**2, dim=1)\n",
    "    # penalize boundary xB\n",
    "    S += torch.sum(\n",
    "            ( (xB - U_2d[:,-1,:]) / dt + grad_V[:,-1,:] )**2, dim=1)\n",
    "    return beta * dt * S\n",
    "\n",
    "def drift(U):\n",
    "    \"\"\" Nonequilibrium drift, currently none. \"\"\"\n",
    "    pass\n",
    "\n",
    "def V(U):\n",
    "    \"\"\" Three-hole potential (weighted sum of Gaussian) \n",
    "    \n",
    "    Inputs:\n",
    "        U,                          (torch.Tensor) (M x N x 2) path samples\n",
    "        \n",
    "    Outputs:\n",
    "        V,                          (M x N) scalar, three-hole potential\n",
    "    \"\"\"\n",
    "    # parameters for the Gaussian, make sure they match with MATLAB\n",
    "    mu1 = torch.tensor([[0., 1./3.]])\n",
    "    mu2 = torch.tensor([[0., 5./3.]])\n",
    "    mu3 = torch.tensor([[-1., 0.]])\n",
    "    mu4 = torch.tensor([[1., 0.]])\n",
    "    A1 = 40.; A2 = -55.; A3 = -50.; A4 = -50.; A5 = 0.2;\n",
    "    # mixture Gaussian with penalty\n",
    "    return g(U, mu1, A1) + g(U, mu2, A2) +\\\n",
    "        g(U, mu3, A3) + g(U, mu4, A4) + 0.2 * torch.sum((U - mu1)**4, dim=-1)\n",
    "    \n",
    "    \n",
    "def g(U, mu, amp):\n",
    "    \"\"\" Gaussian function. \n",
    "    \n",
    "    Inputs:\n",
    "        U,                          (torch.Tensor) (M x N x 2) A path in R^2\n",
    "        mu,                         (torch.Tensor) (1 x 2) mean of Gaussian\n",
    "        amp,                        (scalar) amplitude of Gaussian\n",
    "        \n",
    "    Outputs:\n",
    "        g_U,                        (M x N) result of Gaussian function evaluated at U \n",
    "    \"\"\"\n",
    "    M = U.shape[0]; N = U.shape[1]\n",
    "    return amp * torch.exp(-torch.sum((U - mu)**2, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion for training\n",
    "def loss_kl_3(log_prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" same as LOSS_KL() but:\n",
    "        - log_prior_distrib is predetermined as passed in as a torch.Tensor\n",
    "        - targ_distrib is callable, rather than PyTorch distribution. \n",
    "    \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    #return (log_prior_distrib - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()\n",
    "    return (log_prior_distrib - sum_of_log_jacobians - torch.log(targ_distrib(zk) )).mean()\n",
    "\n",
    "def loss_kl_2(prior_distrib, targ_distrib, z0, zk, log_jacobians):\n",
    "    \"\"\" sae as LOSS_KL() but targ_distrib is callable, rather than PyTorch distribution. \"\"\"\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (prior_distrib.log_prob(z0) - sum_of_log_jacobians - torch.log(targ_distrib(zk) + 1e-40)).mean()\n",
    "\n",
    "\n",
    "# equilibrium PDF used for training\n",
    "def equilibrium_pdf(U, beta=path_beta, dt=path_dt):\n",
    "    \n",
    "    # forcefully truncate the domain, make sure domain \n",
    "    # is consistent with MATLAB\n",
    "    # [-1.2,1.2] x [-0.7,2]\n",
    "    \n",
    "    assert U.shape[1] % 2 == 0, \"Dimension of path must be divisible by 2. \"\n",
    "    # reshape to (B x N x 2), torch reshapes by row, hence the transpose, B is batch size\n",
    "    N = int(U.shape[1] // 2)\n",
    "    U_2d = U.reshape([-1, 2, N])\n",
    "    U_2d = U_2d.transpose(1, 2) # U has size (B x N x 2)\n",
    "    # check if every point for each batch path is in the domain, if not, it should have 0 probability\n",
    "    prob = torch.zeros([U_2d.shape[0]])\n",
    "    valid_prob = torch.exp(-0.25 * path_action(U, beta, dt)) \n",
    "    #for j in range(U_2d.shape[0]):\n",
    "    #    path_j = U_2d[j,:,:]\n",
    "    #    if ( ( path_j[:,0] < -1.5 ).any() ) or ( ( path_j[:,0] > 1.5 ).any() ):\n",
    "    #        prob[j] = 0\n",
    "    #    elif ( ( path_j[:,1] < -0.9 ).any() ) or ( ( path_j[:,1] > -2.2 ).any() ):\n",
    "    #        prob[j] = 0\n",
    "    #    else:\n",
    "    #        prob[j] = valid_prob[j]\n",
    "    return valid_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "which = 5 # which plane are we plotting, plot(dim(D-which-1),dim(D-which))\n",
    "plot_samples = num_samples\n",
    "data_idx = np.random.randint(0, training_data.shape[0], plot_samples)\n",
    "plt.figure(figsize=(20,8));\n",
    "plt.subplot(1,2,1); plt.grid(True); \n",
    "plt.xlabel(\"$\\\\theta {}$\".format(transition_dim-which)); \n",
    "plt.ylabel(\"$\\\\theta {}$\".format(transition_dim-which+1)); \n",
    "plt.title(\"Samples from Truncated TT\"); \n",
    "plot_data_trunc = training_data[data_idx, :]\n",
    "\n",
    "plt.hexbin(plot_data_trunc[:,transition_dim-which-1], plot_data_trunc[:,transition_dim-which]);\n",
    "\n",
    "# it should look like two modes\n",
    "plt.hexbin(plot_data_trunc[:,transition_dim-which-1], plot_data_trunc[:,transition_dim-which]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin training \n",
    "flow_type = 'planar_flow'\n",
    "if flow_type == 'planar_flow':\n",
    "    path_flow = NormalizingFlow(dim=transition_dim, blocks=None, flow_length=32, density=None)\n",
    "elif flow_type == 'autoregressive':\n",
    "    raise NotImplementedError(\"Autoregressive Flow needs to be implemented. \")\n",
    "    \n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(path_flow.parameters(), lr=1e-2)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimization loop\n",
    "num_iter = 10000\n",
    "num_subplot = 0\n",
    "all_losses = []\n",
    "batch_size = 2**12\n",
    "\n",
    "\n",
    "# check exists\n",
    "training_data; transition_dim;\n",
    "save_training_data = training_data\n",
    "\n",
    "# As per discussion with Michael, shift the Gaussian by estimated mean \n",
    "# and scale by estimated std in each dimension\n",
    "est_mean = training_data.mean(0)\n",
    "est_std = training_data.std(0)\n",
    "\n",
    "# d-DIMENSIONAL standard normal as toy training base\n",
    "ref_distrib = distrib.MultivariateNormal(torch.Tensor(est_mean), torch.diag(torch.Tensor(est_std)))\n",
    "targ_distrib = distrib.MultivariateNormal(torch.zeros([transition_dim]), torch.eye(transition_dim))\n",
    "\n",
    "\n",
    "use_normal = False # use multivariate normal as base distribution\n",
    "if use_normal:\n",
    "    training_data = ref_distrib.sample((training_data.shape[0], ))\n",
    "else:\n",
    "    training_data = save_training_data\n",
    "\n",
    "# define loss function we are using\n",
    "#criterion = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n",
    "for it in range(num_iter+1):\n",
    "    # Draw a random sample batch from \"base\" (the truncated TT samples, in training data)\n",
    "    # row indices\n",
    "    sample_idx = np.random.choice(training_data.shape[0], batch_size)\n",
    "    samples = torch.Tensor(training_data[sample_idx,:])\n",
    "    #samples = torch.Tensor(training_data)\n",
    "    \n",
    "    # draw from normal and see if it captures Rosenbrock\n",
    "    #samples = ref_distrib.sample((N, ))\n",
    "    # flow this sample\n",
    "    zk, log_jacobians = path_flow(samples)\n",
    "    \n",
    "    if use_normal:\n",
    "        # if we are using normal, the base density is simple to evaluate\n",
    "        log_base_prob = ref_distrib.log_prob(samples).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "\n",
    "    else:\n",
    "        # need evaluating TT density\n",
    "        log_base_prob = torch.Tensor(log_training_data_densities[sample_idx]).reshape(-1,1)\n",
    "        \n",
    "        # volume correction (approximate dist.)\n",
    "        #log_base_prob -= sum(log_jacobians).reshape(-1, 1)\n",
    "        log_base_prob = log_base_prob.reshape(-1)\n",
    "        base_prob = log_base_prob\n",
    "        \n",
    "    ### visual reporting\n",
    "    if (it % 1000 == 0):\n",
    "        #print(rosen_planar_flow.weight)\n",
    "        # plot the flowed samples\n",
    "        num_subplot += 1\n",
    "        plt.figure(2, figsize=(8,6));\n",
    "        plt.grid(True); \n",
    "        plt.xlabel(\"$dim {}$\".format(transition_dim-1)); \n",
    "        plt.ylabel(\"$dim {}$\".format(transition_dim)); \n",
    "        plt.title(\"{} at iter. {}\".format(num_subplot, it));\n",
    "        # pick a sample from training data, flow, and plot it\n",
    "        data_idx = np.random.randint(0, training_data.shape[0], num_samples)\n",
    "        plot_data = torch.Tensor(training_data[data_idx, :])\n",
    "        flowed_plot_data, _ = path_flow(plot_data)\n",
    "        flowed_plot_data = flowed_plot_data.detach().numpy()\n",
    "        plt.hexbin(flowed_plot_data[:,transition_dim-2], flowed_plot_data[:,transition_dim-1]);\n",
    "        # save figure\n",
    "        plt.savefig(\"./img/twochannel/transition_path_batchsz{}_train_iter{}\".format(batch_size, it))\n",
    "        #plt.show()\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # compute loss of on the flowed sample\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = loss_kl_3(base_prob, equilibrium_pdf, samples, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 500 == 0):\n",
    "        #print('Log ML Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        print('KL Divergence Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # for plotting loss\n",
    "        all_losses.append(loss_v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot samples after correction\n",
    "path_samples_corrected = path_flow(torch.Tensor(training_data))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_samples_corrected = path_samples_corrected.detach().numpy().reshape([num_samples, 2, \\\n",
    "                                                                          transition_dim // 2]).transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    plt.plot(path_samples_corrected[i,:,0], path_samples_corrected[i,:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections using 4d Gaussian (old example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# load 4d sample data\n",
    "data = scipy.io.loadmat(\"./tt_irt_sample.mat\")\n",
    "data.keys()\n",
    "\n",
    "# load a few useful variables\"\n",
    "unif_sample = data['unif_sample']\n",
    "grid = data['x']\n",
    "# Gaussian parameters used to generate data\n",
    "mu = data['mu']\n",
    "sigma = data['sigma']\n",
    "d = data['d'][0][0]\n",
    "# generated samples\n",
    "sample_exact = data['s_exact'] # can only be used for comparison\n",
    "sample_tt = data['xq'] # sample from TT-IRT\n",
    "print(\"=== Ftt rank before truncation: \", data['ftt_rank_before'])\n",
    "print(\"== Ftt rank after truncation: \", data['ftt_rank_after'])\n",
    "print(\"=== data size: \", sample_tt.shape)\n",
    "print(\"=== current Wasserstein metric: \", scipy.stats.wasserstein_distance(sample_exact.flatten(), \\\n",
    "                                                                           sample_tt.flatten()))\n",
    "\n",
    "# Create normalizing flow\n",
    "flow = NormalizingFlow(dim=d, flow_length=16, density=None)\n",
    "\n",
    "def loss(distrib, zk, log_jacobians):\n",
    "    sum_of_log_jacobians = sum(log_jacobians)\n",
    "    return (-sum_of_log_jacobians - distrib.log_prob(zk)+1e-9).mean()\n",
    "\n",
    "import torch.optim as optim\n",
    "# Create optimizer algorithm\n",
    "optimizer = optim.Adam(flow.parameters(), lr=2e-3)\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# exact prob density\n",
    "target_distrib = distrib.MultivariateNormal(torch.Tensor(mu.T), torch.Tensor(sigma))\n",
    "\n",
    "\n",
    "# we use the sample from severely truncated TT\n",
    "\n",
    "\n",
    "#id_figure=2\n",
    "#plt.figure(figsize=(16, 18))\n",
    "\n",
    "# Main optimization loop\n",
    "for it in range(10001):\n",
    "    # Draw a sample batch from TT sample\n",
    "    #sample_idx = np.random.randint(0, sample_tt.shape[0], 1024)\n",
    "    #samples = torch.Tensor(sample_tt[sample_idx, :])\n",
    "    samples = torch.Tensor(sample_tt)\n",
    "    # Evaluate flow of transforms\n",
    "    zk, log_jacobians = flow(samples)\n",
    "    # Evaluate loss and backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = loss(target_distrib, zk, log_jacobians)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if (it % 500 == 0):\n",
    "        print('Loss (it. %i) : %f'%(it, loss_v.item()))\n",
    "        # Draw random samples\n",
    "        #samples = ref_distrib.sample((int(1e5), ))\n",
    "        # Evaluate flow and plot\n",
    "        #zk, _ = flow(samples)\n",
    "        #zk = zk.detach().numpy()\n",
    "        #plt.subplot(3,4,id_figure)\n",
    "        #plt.hexbin(zk[:,0], zk[:,1], cmap='rainbow')\n",
    "        #plt.title('Iter.%i'%(it), fontsize=15);\n",
    "        #id_figure += 1\n",
    "\n",
    "# check after training\n",
    "samples_corrected, _ = flow(torch.Tensor(sample_tt))\n",
    "samples_corrected = samples_corrected.detach().numpy()\n",
    "print(\"=== Wasserstein metric (corrected): \", scipy.stats.wasserstein_distance(sample_exact.flatten(), \\\n",
    "                                                                           samples_corrected.flatten()))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 50d Rosen sample data\n",
    "data_ginz = scipy.io.loadmat(\"./data/tt_irt_ginzburg1d_sample.mat\")\n",
    "display(data_ginz.keys())\n",
    "# very high precision\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data from truncated TT\n",
    "training_data = data_ginz['training_data_ginsburg']\n",
    "log_training_data_densities = data_ginz['training_densities_ginsburg']\n",
    "# used for comparison, from accurate TT\n",
    "comparison_data = data_ginz['comparison_data_ginsburg']\n",
    "comparison_data_densities = data_ginz['comparison_densities_ginsburg']\n",
    "print(\"====== shape of training data = {}\".format(training_data.shape))\n",
    "num_samples = data_ginz['N'][0][0]\n",
    "num_epoch = data_ginz['epoch'][0][0]\n",
    "# seed used to generate samples\n",
    "unif_seed = data_ginz['unif_sample']\n",
    "# dimension of the PDF\n",
    "ginz_dim = unif_seed.shape[1]\n",
    "# normalization constant\n",
    "Z_beta = data_ginz['Z_beta'][0][0]\n",
    "# parameters\n",
    "temperature = data_ginz['temp'][0][0]\n",
    "ginz_delta = data_ginz['delta'][0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
